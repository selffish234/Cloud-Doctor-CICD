"""
AI Engine - Gemini 2.5ë¥¼ í™œìš©í•œ ë¡œê·¸ ë¶„ì„
GCP Vertex AIë¥¼ í†µí•´ AWS ë¡œê·¸ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
ë©”ê°€ì¡´í´ë¼ìš°ë“œ ì±„ìš© í¬ì¸íŠ¸: AI ê¸°ë°˜ ìë™í™” ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
"""

import logging
from typing import List, Dict, Any
import json

from google.cloud import aiplatform
from vertexai.generative_models import GenerativeModel, Content, Part
import vertexai

logger = logging.getLogger(__name__)


class GeminiAnalyzer:
    """
    Gemini 2.5 Flashë¥¼ ì‚¬ìš©í•œ ë¡œê·¸ ë¶„ì„ ì—”ì§„

    íŠ¹ì§•:
    - ë‹¤ìˆ˜ì˜ ì—ëŸ¬ ë¡œê·¸ë¥¼ í•œ ë²ˆì— ë¶„ì„
    - ê·¼ë³¸ ì›ì¸ ì‹ë³„ ë° í•´ê²°ì±… ì œì‹œ
    - ìš°ì„ ìˆœìœ„ íŒë‹¨ (Critical, High, Medium, Low)
    """

    def __init__(
        self,
        project_id: str,
        location: str = "us-central1",
        model_name: str = "gemini-2.0-flash-exp"  # ë˜ëŠ” gemini-1.5-flash
    ):
        """
        Args:
            project_id: GCP í”„ë¡œì íŠ¸ ID
            location: Vertex AI ë¦¬ì „
            model_name: ì‚¬ìš©í•  Gemini ëª¨ë¸ëª…
        """
        self.project_id = project_id
        self.location = location
        self.model_name = model_name

        # Vertex AI ì´ˆê¸°í™”
        vertexai.init(project=project_id, location=location)
        self.model = GenerativeModel(model_name)

        logger.info(f"ğŸ¤– Gemini AI Engine initialized: {model_name}")

    def _create_analysis_prompt(self, logs: List[Dict[str, Any]]) -> str:
        """
        ë¡œê·¸ ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±

        ë©”ê°€ì¡´í´ë¼ìš°ë“œ ì±„ìš© í¬ì¸íŠ¸:
        - Prompt Engineeringì„ í†µí•œ ì •í™•í•œ ë¶„ì„ ìœ ë„
        - êµ¬ì¡°í™”ëœ ì¶œë ¥ (JSON) ìš”ì²­
        """
        # ë¡œê·¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        log_text = "\n".join([
            f"[{log['timestamp']}] {log['message']}"
            for log in logs
        ])

        prompt = f"""
ë‹¹ì‹ ì€ í´ë¼ìš°ë“œ ì¸í”„ë¼ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ AWS CloudWatch ë¡œê·¸ë¥¼ ë¶„ì„í•˜ì—¬ ë¬¸ì œë¥¼ ì§„ë‹¨í•´ ì£¼ì„¸ìš”.

## ë¡œê·¸ ë°ì´í„°
```
{log_text}
```

## ë¶„ì„ ìš”êµ¬ì‚¬í•­
ë‹¤ìŒ í˜•ì‹ì˜ JSONìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”:

{{
  "summary": "ì „ì²´ ë¡œê·¸ì— ëŒ€í•œ í•œ ì¤„ ìš”ì•½ (í•œê¸€, 50ì ì´ë‚´)",
  "severity": "CRITICAL|HIGH|MEDIUM|LOW ì¤‘ í•˜ë‚˜",
  "issues": [
    {{
      "type": "ì—ëŸ¬ ìœ í˜• (ì˜ˆ: Database Connection Failure)",
      "count": ë¡œê·¸ì—ì„œ ë°œê²¬ëœ íšŸìˆ˜,
      "description": "ë¬¸ì œ ì„¤ëª… (í•œê¸€, 100ì ì´ë‚´)",
      "root_cause": "ê·¼ë³¸ ì›ì¸ ì¶”ì • (í•œê¸€)",
      "solution": "í•´ê²° ë°©ë²• ì œì•ˆ (í•œê¸€, êµ¬ì²´ì ìœ¼ë¡œ)"
    }}
  ],
  "priority_actions": [
    "ìš°ì„ ì ìœ¼ë¡œ í•´ì•¼ í•  ì¡°ì¹˜ 1",
    "ìš°ì„ ì ìœ¼ë¡œ í•´ì•¼ í•  ì¡°ì¹˜ 2",
    "ìš°ì„ ì ìœ¼ë¡œ í•´ì•¼ í•  ì¡°ì¹˜ 3"
  ],
  "technical_keywords": ["ê´€ë ¨ëœ", "ê¸°ìˆ ", "í‚¤ì›Œë“œ", "ë¦¬ìŠ¤íŠ¸"]
}}

## ì¤‘ìš” ì‚¬í•­
- ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš” (ì¶”ê°€ ì„¤ëª… ì—†ì´)
- ì—ëŸ¬ê°€ ì—¬ëŸ¬ ì¢…ë¥˜ë¼ë©´ issues ë°°ì—´ì— ëª¨ë‘ í¬í•¨í•˜ì„¸ìš”
- ìš°ì„ ìˆœìœ„ëŠ” ì˜í–¥ë„ì™€ ê¸´ê¸‰ì„±ì„ ê³ ë ¤í•˜ì„¸ìš”
- í•œê¸€ë¡œ ëª…í™•í•˜ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”
"""
        return prompt

    async def analyze_logs(self, logs: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ë¡œê·¸ë¥¼ Gemini AIë¡œ ë¶„ì„í•©ë‹ˆë‹¤.

        Args:
            logs: ë¡œê·¸ ì´ë²¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            if not logs:
                return {
                    "summary": "ë¶„ì„í•  ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤",
                    "severity": "LOW",
                    "issues": [],
                    "priority_actions": []
                }

            logger.info(f"ğŸ¤– Analyzing {len(logs)} logs with Gemini AI...")

            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._create_analysis_prompt(logs)

            # Gemini API í˜¸ì¶œ
            response = self.model.generate_content(
                prompt,
                generation_config={
                    "temperature": 0.2,  # ì¼ê´€ì„± ìˆëŠ” ë¶„ì„ì„ ìœ„í•´ ë‚®ì€ temperature
                    "top_p": 0.8,
                    "top_k": 40,
                    "max_output_tokens": 2048,
                }
            )

            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            response_text = response.text.strip()

            logger.info(f"ğŸ“ Raw Gemini Response:\n{response_text[:500]}...")

            # JSON íŒŒì‹± ì‹œë„
            try:
                # JSON ì½”ë“œ ë¸”ë¡ ì œê±° (Geminiê°€ ```json ... ``` í˜•íƒœë¡œ ë°˜í™˜í•  ìˆ˜ ìˆìŒ)
                if response_text.startswith("```json"):
                    response_text = response_text.split("```json")[1].split("```")[0].strip()
                elif response_text.startswith("```"):
                    response_text = response_text.split("```")[1].split("```")[0].strip()

                analysis_result = json.loads(response_text)
                logger.info("âœ… Successfully parsed AI analysis result")

                return analysis_result

            except json.JSONDecodeError as e:
                logger.error(f"âŒ Failed to parse JSON response: {str(e)}")
                logger.error(f"   Raw response: {response_text}")

                # Fallback: ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
                return {
                    "summary": "AI ë¶„ì„ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨",
                    "severity": "UNKNOWN",
                    "issues": [{
                        "type": "Analysis Error",
                        "count": 0,
                        "description": "AI ì‘ë‹µì„ JSONìœ¼ë¡œ ë³€í™˜í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤",
                        "root_cause": "ì‘ë‹µ í˜•ì‹ ë¶ˆì¼ì¹˜",
                        "solution": "í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜ ëª¨ë¸ì„ ì¬ì‹œë„í•˜ì„¸ìš”"
                    }],
                    "priority_actions": ["AI ë¶„ì„ ì¬ì‹œë„"],
                    "raw_response": response_text[:500]
                }

        except Exception as e:
            logger.error(f"âŒ AI analysis failed: {str(e)}", exc_info=True)

            return {
                "summary": "AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
                "severity": "UNKNOWN",
                "issues": [{
                    "type": "System Error",
                    "count": 0,
                    "description": str(e),
                    "root_cause": "AI ì—”ì§„ ì˜¤ë¥˜",
                    "solution": "ì‹œìŠ¤í…œ ë¡œê·¸ë¥¼ í™•ì¸í•˜ê³  ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”"
                }],
                "priority_actions": ["ì‹œìŠ¤í…œ ì ê²€ í•„ìš”"]
            }

    async def analyze_single_log(self, log_message: str) -> str:
        """
        ë‹¨ì¼ ë¡œê·¸ ë©”ì‹œì§€ì— ëŒ€í•œ ê°„ë‹¨í•œ ë¶„ì„ (ë¹ ë¥¸ ì§„ë‹¨ìš©)

        Args:
            log_message: ë¡œê·¸ ë©”ì‹œì§€ í…ìŠ¤íŠ¸

        Returns:
            í•œ ë¬¸ì¥ ìš”ì•½
        """
        try:
            prompt = f"""
ë‹¤ìŒ ì—ëŸ¬ ë¡œê·¸ë¥¼ ë³´ê³ , ì›ì¸ê³¼ í•´ê²°ì±…ì„ **í•œ ë¬¸ì¥**ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.

ë¡œê·¸: {log_message}

í˜•ì‹: "[ì›ì¸] ... ë•Œë¬¸ì— ë°œìƒ. [í•´ê²°] ... í•˜ì„¸ìš”."
"""

            response = self.model.generate_content(
                prompt,
                generation_config={
                    "temperature": 0.1,
                    "max_output_tokens": 200
                }
            )

            return response.text.strip()

        except Exception as e:
            logger.error(f"Single log analysis failed: {str(e)}")
            return f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}"

    def format_for_slack(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        ë¶„ì„ ê²°ê³¼ë¥¼ Slack Block Kit í˜•ì‹ìœ¼ë¡œ ë³€í™˜

        Args:
            analysis: analyze_logs() ê²°ê³¼

        Returns:
            Slack message payload
        """
        severity_emoji = {
            "CRITICAL": "ğŸ”´",
            "HIGH": "ğŸŸ ",
            "MEDIUM": "ğŸŸ¡",
            "LOW": "ğŸŸ¢",
            "UNKNOWN": "âšª"
        }

        emoji = severity_emoji.get(analysis.get("severity", "UNKNOWN"), "âšª")

        blocks = [
            {
                "type": "header",
                "text": {
                    "type": "plain_text",
                    "text": f"{emoji} Cloud Doctor ì§„ë‹¨ ê²°ê³¼"
                }
            },
            {
                "type": "section",
                "fields": [
                    {
                        "type": "mrkdwn",
                        "text": f"*ì‹¬ê°ë„:*\n{analysis.get('severity', 'UNKNOWN')}"
                    },
                    {
                        "type": "mrkdwn",
                        "text": f"*ìš”ì•½:*\n{analysis.get('summary', 'N/A')}"
                    }
                ]
            },
            {"type": "divider"}
        ]

        # ë°œê²¬ëœ ì´ìŠˆë“¤
        for i, issue in enumerate(analysis.get("issues", []), 1):
            blocks.append({
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"*Issue #{i}: {issue.get('type')}*\n"
                            f"â€¢ ë°œìƒ íšŸìˆ˜: {issue.get('count')}íšŒ\n"
                            f"â€¢ ì›ì¸: {issue.get('root_cause', 'N/A')}\n"
                            f"â€¢ í•´ê²°ì±…: {issue.get('solution', 'N/A')}"
                }
            })

        # ìš°ì„  ì¡°ì¹˜ ì‚¬í•­
        if analysis.get("priority_actions"):
            actions_text = "\n".join([
                f"{i}. {action}"
                for i, action in enumerate(analysis["priority_actions"], 1)
            ])

            blocks.append({"type": "divider"})
            blocks.append({
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"*ğŸ¯ ìš°ì„  ì¡°ì¹˜ ì‚¬í•­:*\n{actions_text}"
                }
            })

        return {
            "blocks": blocks
        }


# ì‚¬ìš© ì˜ˆì‹œ (í…ŒìŠ¤íŠ¸ìš©)
if __name__ == "__main__":
    import asyncio
    import os

    async def test():
        analyzer = GeminiAnalyzer(
            project_id=os.getenv("GCP_PROJECT_ID", "your-project-id"),
            location="us-central1"
        )

        # ìƒ˜í”Œ ë¡œê·¸
        sample_logs = [
            {
                "timestamp": "2024-01-10T10:30:15",
                "message": "[ERROR] Connection refused: Could not connect to database at 10.0.2.55"
            },
            {
                "timestamp": "2024-01-10T10:30:45",
                "message": "[ERROR] SQLSTATE[HY000] [2002] Connection timed out after 30s"
            },
            {
                "timestamp": "2024-01-10T10:31:20",
                "message": "[MEMORY ERROR] OutOfMemoryError: Java heap space exceeded"
            }
        ]

        print("Analyzing sample logs...")
        result = await analyzer.analyze_logs(sample_logs)
        print(json.dumps(result, indent=2, ensure_ascii=False))

        print("\n" + "="*60)
        print("Slack format:")
        slack_msg = analyzer.format_for_slack(result)
        print(json.dumps(slack_msg, indent=2, ensure_ascii=False))

    asyncio.run(test())
